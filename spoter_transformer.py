# -*- coding: utf-8 -*-
"""nathaniel_SPOTER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QCzM50LIhAA8xGa9YnWqUw94NVGFAsG4

Transformer Code
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount('/content/drive', force_remount=True)
# %cd './drive/Shareddrives/CS231A_proj/CS231A'

!pip install pytorch_lightning
!pip install ujson

# from utils import augmentations
from utils.dataloader import ASLLVDataset, collate_fn
from utils.spoter import SPOTER
import pytorch_lightning as pl
import torchmetrics
import torch 
import torch.nn as nn
from torch.utils.data import DataLoader
import pandas as pd

from sklearn import preprocessing
from sklearn.model_selection import train_test_split


batch_size = 1
annotations_file = '/content/drive/MyDrive/CS231A/asllvd_shorter_data.csv'
pose_dir = '/content/drive/MyDrive/CS231A/ASLLVD/output_json/'
normal_pose_dir = '/content/drive/MyDrive/CS231A/ASLLVD/normalized_json/'

#TODO: TEST THAT SPLIT WORKS 
# sTRATIFY PROBABLY BREAKS, REMOVE GLOSSES WITH < 3 EXAMPLES
df = pd.read_csv(annotations_file)

le = preprocessing.LabelEncoder()

train, val_test = train_test_split(df, test_size=0.6, stratify=df['label index'])
val, test = train_test_split(val_test, test_size=0.5, stratify=val_test['label index'])

train_ds = ASLLVDataset(train, normal_pose_dir, keep_uncertainty=True, train=True)
val_ds = ASLLVDataset(val, normal_pose_dir, keep_uncertainty=True, train=False)
test_ds = ASLLVDataset(test, normal_pose_dir, keep_uncertainty=True, train=False)

train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)



for batch in train_dl:
  data, labels = batch
  print(data[0].shape)
  break

class ModelWrapper(pl.LightningModule):
    def __init__(self, 
                 n_feats=64, 
                 hidden_dim=32, 
                 out_feats=32, 
                 num_classes=6,
                 batch_size=16,
                 loss_fn = nn.CrossEntropyLoss):
      
        super().__init__()
        self.model = SPOTER(
            num_classes = num_classes,
            hidden_dim = hidden_dim
        )
        
        self.loss = loss_fn()
        self.batch_size = batch_size
        self.num_classes = num_classes

        self.training_step_outputs = []
        self.training_step_labels = []
        self.valid_step_outputs = []
        self.valid_step_labels = []
        self.test_step_outputs = []
        self.test_step_labels = []
        self.accuracy = torchmetrics.Accuracy('multiclass', num_classes=self.num_classes)
        self.micro_f1 = torchmetrics.F1Score(task='multiclass', num_classes=self.num_classes, average='micro')
        self.macro_f1 = torchmetrics.F1Score(task='multiclass', num_classes=self.num_classes, average='macro')


    def configure_optimizers(self):
      return torch.optim.Adam(lr = 0.001)

    def log_metrics(self, y, preds, mode='train'):
      '''
      y:      True data labels
      preds:  Predicted data labels
      mode:   Either 'train' or 'valid' or 'test'. For logging
      '''
      micro_f1 = self.micro_f1(preds, y)
      self.log(f'f1/{mode}/micro_f1', micro_f1)
      macro_f1 = self.macro_f1(preds, y)
      self.log(f'f1/{mode}/macro_f1', macro_f1)

      accuracy = self.accuracy(torch.argmax(preds, dim=1), y)
      self.log(f'accuracy/{mode}', accuracy)
      # for label in torch.unique(y):
      #   y_sub = y[torch.where(y == label)]
      #   pred_sub = preds[torch.where(y == label)]
      #   class_acc = self.accuracy(torch.argmax(pred_sub, dim=1), y_sub)
      #   self.log(f'class_acc/{mode}/accuracy_{label}', class_acc)


    # --- Forward pass
    def forward(self, x):
        out = self.model(x)

        return out

    def training_step(self, batch, batch_idx) -> torch.Tensor:
       #TODO: FIX X AND Y TO BE ACTUAL VALUES
        x, y = batch
        x = x[0]
        y = y[0]

        y_hat = self(x).squeeze()
        
        self.training_step_outputs.append(y_hat)
        self.training_step_labels.append(y)
        loss = self.loss(y_hat, y)

        return loss

    def on_train_epoch_end(self):
        all_preds = torch.stack(self.training_step_outputs, dim=0)
        all_labels = torch.stack(self.training_step_labels, dim=0)

        loss = self.loss(all_preds, all_labels)
        self.log('loss/train_loss', loss)
        self.log_metrics(all_labels, all_preds, 'train')
        
        self.training_step_outputs.clear()
        self.training_step_labels.clear()
      

    def validation_step(self, batch, batch_idx):
        x, y = batch
        x = x[0]
        y = y[0]

        y_hat = self(x).squeeze()
        self.valid_step_outputs.append(y_hat)
        self.valid_step_labels.append(y)
        # self.log_metrics(y, y_hat, 'val')
        loss = self.loss(y_hat, y)
        # self.log('loss/val_loss', loss)
        return loss

    def on_validation_epoch_end(self):
        all_preds = torch.stack(self.valid_step_outputs, dim=0)
        all_labels = torch.stack(self.valid_step_labels, dim=0)

        loss = self.loss(all_preds, all_labels)
        self.log('loss/valid_loss', loss)
        self.log_metrics(all_labels, all_preds, 'valid')
        
        self.valid_step_outputs.clear()
        self.valid_step_labels.clear()
        

    def test_step(self, batch, batch_idx):
      # TODO: FIX X AND Y TO BE aCTUAL VALUES
        x, y = batch
        x = x[0]
        y = y[0]

        y_hat = self(x).squeeze()
        self.test_step_outputs.append(y_hat)
        self.test_step_labels.append(y)
        loss = self.loss(y_hat, y)

        self.log("test_loss", loss)
        return loss

    def on_test_epoch_end(self):
        all_preds = torch.stack(self.test_step_outputs, dim=0)
        all_labels = torch.stack(self.test_step_labels, dim=0)

        loss = self.loss(all_preds, all_labels)
        self.log('loss/test_loss', loss)
        self.log_metrics(all_labels, all_preds, 'test')
        
        self.test_step_outputs.clear()
        self.test_step_labels.clear() 

    def configure_optimizers(self) -> torch.optim.Optimizer:
        return torch.optim.Adam(params=self.parameters(), lr=0.001)

trainer = pl.Trainer(
    # strategy=None,
    accelerator='gpu',
    devices=1,
    benchmark=True,
    deterministic=False,
    num_sanity_val_steps=0,
    max_epochs=3,
    log_every_n_steps=1
)

# TODO: FIX ARGUMENTS
model = ModelWrapper(n_feats=411, 
                  hidden_dim=272, 
                  num_classes=425,
                  batch_size=batch_size,
                  loss_fn = nn.CrossEntropyLoss)
# model.to('cuda')
trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %rm -rf "/content/lightning_logs"
# %tensorboard --bind_all --logdir "lightning_logs" --port=6008

"""# Data Loader"""